## 什么是线性回归

- 在统计学中，线性回归是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。当因变量和自变量之间高度相关时，我们就可以使用线性回归来对数据进行预测。
- **线性回归模型，是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。**其表达形式为y =  w'x+e，e为误差服从均值为0的正态分布。线性回归模型是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归,大于一个自变量情况的叫做多元回归。

## **简单线性回归的原理及数学推导**

简单线性回归为一元线性回归模型，是指模型中只含有一个自变量和因变量，该模型的数学公式可以表示为y=ax+b+ε，a为模型的斜率，b为模型的截距，ε为误差。

我们可能听过最小二乘法(OLS)这个词，它就是我们理解简单线性回归模型的关键。最小二乘法，其实就是最小化残差的平方和。

对于简单线性回归，函数可以用可以用一个公式来表示，假设x和y之间存在这样的关系：

 ![img](https://pic4.zhimg.com/80/v2-b1ebd51aa44b6f1156ea96e486aa3013_720w.png) 

 其中等号左边带的y是我们y的预测值，实际情况时我们预测的数值与实际数值之间的差异，就是残差，即：  ![img](https://pic4.zhimg.com/80/v2-6439351e31d9cc3295fbb03934abe3e7_720w.png)  残差平方和，就是：  ![img](https://pic1.zhimg.com/80/v2-c89ef73c8bd32d7ce92d76c51496d90c_720w.png) 求导（最小二乘法）、梯度下降两种方式，

那么接下来我们就来求解我们的系数(a, b)，如前边所言，残差平方和的数据形态是一个U型曲线，因此当且仅当导数为0时，我们得到最低点，重点是导数为0时，此时为残差平方和曲线的最低点，即此时的残差最小，结果最接近实际值。

接下来就是运用导数的知识求解这个数学公式。

**求解过程：求导--->导数结果为0时为正确结果--->求出a、b的计算公式**

1. 求解b

J(a, b)代表损失函数，在这个例子中，就是我们上边提到的残差平方和。先对b求导，并使导数等于0： ![img](https://pic1.zhimg.com/80/v2-b3c84ab7d19634733a4d9f9001f94844_720w.jpg)  ![img](https://pic3.zhimg.com/80/v2-3370c81a9fbf36510f0e94dde78396e2_720w.jpg)  ![img](https://pic4.zhimg.com/80/v2-42f29b8f3dfafe54f0a96fa95d9587bf_720w.jpg) 

```
# 首先构造一组数据，然后画图
 
import numpy as np
import matplotlib.pyplot as plt
 
x = np.array([1.,2.,3.,4.,5.])
y = np.array([1.,3.,2.,3.,5,])
 
plt.scatter(x,y)
plt.axis([0,6,0,6])
plt.show()
 
# 实际上，同一组数据，选择不同的f(x)，即模型
# 通过最小二乘法可以得到不一样的拟合曲线
 
# 不同的数据，更可以选择不同的函数
# 通过最小二乘法可以得到不一样的拟合曲线
 
# 用最小二乘法假设模型，再求出参数
 
# 首先要计算x和y的均值
x_mean = np.mean(x)
y_mean = np.mean(y)
 
# a的分子num、分母d
num = 0.0
d = 0.0
for x_i,y_i in zip(x,y):   # zip函数打包成[(x_i,y_i)...]的形式
    num = num + (x_i - x_mean) * (y_i - y_mean)
    d = d + (x_i - x_mean) ** 2
a = num / d
b = y_mean - a * x_mean
 
# 在求出a、b之后，可以计算出y的预测值，首先绘制模型直线：
y_hat = a * x + b
 
plt.scatter(x,y)    # 绘制散点图
plt.plot(x,y_hat,color='r')    # 绘制直线
plt.axis([0,6,0,6])
plt.show()
 
# 然后进行预测：
x_predict = 6
y_predict = a * x_predict + b
print(y_predict)

```

对于下面的样本数据集 对应的是一个向量，每一行是一个样本，每列对应一个特征。对应的结果可以用如下如下公式： 

简单线性回归，只计算前两项，但是在多元线性回归中就要学习到n+1个参数，就能求出多元线性回归预测值： 

也就是：第一个特征与参数1相乘、第二个特征与参数2相乘，累加之后再加上截距。就能得到预测值。



```
import numpy as np
from .metrics import r2_score

class LinearRegression:

    def __init__(self):
        """初始化Linear Regression模型"""
        self.coef_ = None    # 系数（theta0~1 向量）
        self.interception_ = None   # 截距（theta0 数）
        self._theta = None  # 整体计算出的向量theta

    def fit_normal(self, X_train, y_train):
        """根据训练数据X_train，y_train训练Linear Regression模型"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"
        # 正规化方程求解
        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)

        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]
        return self

    def predict(self, X_predict):
        """给定待预测的数据集X_predict，返回表示X_predict的结果向量"""
        assert self.interception_ is not None and self.coef_ is not None, \
            "must fit before predict"
        assert X_predict.shape[1] == len(self.coef_), \
            "the feature number of X_predict must be equal to X_train"
        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        y_predict = X_b.dot(self._theta)
        return y_predict

    def score(self, X_test, y_test):
        
        y_predict = self.predict(self, X_test)
        return r2_score(y_test, y_predict)
    

    def __repr__(self):
        return "LinearRegression()"
```

