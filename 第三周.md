1. ### min-max归一化

   该方法是对原始数据进行线性变换，将其映射到[0,1]之间,该方法也被称为离差标准化(但是请注意，网上更多人喜欢把z-score称为标准化方法，把min-max称为归一化方法，然后在此基础上，强行给标准化(z-score)与归一化(min-max)划条界线，以显示二者之间的相异性。对！二者之间确实有很大的不同，这个我们后面会有介绍，但是这两个方法说到底还都是用来去除量纲的，都是无量纲化技术中的一员而已，所以，请不要纠结标准化与归一化这两个概念了)。

   ​              ![img](https://img-blog.csdn.net/20180409212228367?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09uVGhlV2F5R29Hb2luZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) 

   上式中，min是样本的最小值，max是样本的最大值。由于最大值与最小值可能是动态变化的，同时也非常容易受噪声(异常点、离群点)影响，因此一般适合小数据的场景。此外，该方法还有两点好处：

   1) 如果某属性/特征的方差很小，如身高：np.array([[1.70],[1.71],[1.72],[1.70],[1.73]])，实际5条数据在身高这个特征上是有差异的，但是却很微弱，这样不利于模型的学习，进行min-max归一化后为：array([[ 0. ], [ 0.33333333], [ 0.66666667], [ 0. ], [ 1. ]])，相当于放大了差异；

   2) 维持稀疏矩阵中为0的条目。

   使用方法如下：

    

   ```
   from sklearn.preprocessing import MinMaxScaler
    x = np.array([[1,-1,2],[2,0,0],[0,1,-1]])
    x1 = MinMaxScaler().fit_transform(x)
   ```

   不难发现，x1每列的值都在[0,1]之间，也就是说，该模块是按列计算的。并且MinMaxScaler在构造类对象的时候也可以直接指定最大最小值的范围：scaler = MinMaxScaler(feature_range=(min, max)).

   2. ### z-score标准化

   z-score标准化(zero-mena normalization，0-均值标准化) ![img](https://img-blog.csdn.net/20180409201129216?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09uVGhlV2F5R29Hb2luZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)  上式中，*x*是原始数据，*u*是样本均值，*σ*是样本标准差。回顾下正态分布的基本性质，若*x*～*N*(*u*,*σ*^2),则有  ![img](https://img-blog.csdn.net/20180409201857970?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09uVGhlV2F5R29Hb2luZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) 其中，N(0,1)表示标准正态分布

      于是，可以看出，z-score标准化方法试图将原始数据集标准化成均值为0，方差为1且接近于标准正态分布的数据集。然而，一旦原始数据的分布 不 接近于一般正态分布，则标准化的效果会不好。该方法比较适合数据量大的场景(即样本足够多，现在都流行大数据，因此可以比较放心地用)。此外，相对于min-max归一化方法，该方法不仅能够去除量纲，还能够把所有维度的变量一视同仁(因为每个维度都服从均值为0、方差1的正态分布)，在最后计算距离时各个维度数据发挥了相同的作用，避免了不同量纲的选取对距离计算产生的巨大影响。所以，涉及到计算点与点之间的距离，如利用距离度量来计算相似度、PCA、LDA，聚类分析等，并且数据量大(近似正态分布)，可考虑该方法。相反地，如果想保留原始数据中由标准差所反映的潜在权重关系应该选择min-max归一化

   ```python
   from sklearn.preprocessing import Standard Scaler
   x = np.array([[1,2,3],[4,5,6],[1,2,1]])
   x1 = StandardScaler().fit_transform(x)
   ```

    可以发现，x1的每一列加起来都是0，方差是1左右。注意该方法同样按列(即每个属性/特征)进行计算。并且StandardScaler类还有一个好处，就是可以直接调用其对象的.mean_与.std_方法查看原始数据的均值与标准差。 

 3.  ### Normalization

          from sklearn import preprocessing
          normalizer = preprocessing.Normalizer().fit(X)
          normalizer.transform(X)

   其实这个方法是根据范数来进行 Normalization的，何为范数？听着感觉高大上，其实非常常见。Lp-范数的计算公式如下所示：


 ![img](https://img-blog.csdn.net/2018041013501184?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09uVGhlV2F5R29Hb2luZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)  可见，*L*2范数即为欧式距离，则规则为*L*2的Normalization公式如下所示，易知，其将每行(条)数据转为相应的“单位向量”。  ![img](https://img-blog.csdn.net/20180410135658551?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09uVGhlV2F5R29Hb2luZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)  Normalization的过程是将每个样本缩放到单位范数(结合单位向量进行理解，*p*=2时为单位向量，其他为单位范数)，如果后面要使用如二次型（点积）或者其它核方法计算两个样本之间的相似性这个方法会很有用 

- ## 缺失值处理

### 1. 直接删除

```
import numpy as np
import pandas as pd
data = pd.read_csv('data.csv',encoding='GBK')
# 将空值形式的缺失值转换成可识别的类型
data = data.replace(' ', np.NaN)
print(data.columns)#['id', 'label', 'a', 'b', 'c', 'd']
#将每列中缺失值的个数统计出来
null_all = data.isnull().sum()
#id       0
#label    0
#a        7
#b        3
#c        3
#d        8
#查看a列有缺失值的数据
a_null = data[pd.isnull(data['a'])]
#a列缺失占比
a_ratio = len(data[pd.isnull(data['a'])])/len(data) #0.0007
#丢弃缺失值,将存在缺失值的行丢失
new_drop = data.dropna(axis=0)
print(new_drop.shape)#(9981,6)

#丢弃某几列有缺失值的行
new_drop2 = data.dropna(axis=0, subset=['a','b'])
print(new_drop2.shape)#(9990,6)

```

## 2.使用一个全局常量填充缺失值



```
#用0填充缺失值
fill_data = data.fillna('Unknow')
print(fill_data.isnull().sum())
#out 
id       0
label    0
a        0
b        0
c        0
d        0

```



## 3.均值、众数、中位数填充

```
#均值填充
data['a'] = data['a'].fillna(data['a'].means())
#中位数填充
data['a'] = data['a'].fillna(data['a'].median())
#众数填充
data['a'] = data['a'].fillna(stats.mode(data['a'])[0][0])
#用前一个数据进行填充
data['a'] = data['a'].fillna(method='pad')
#用后一个数据进行填充
data['a'] = data['a'].fillna(method='bfill')

```



## 4. 插值法、KNN填充

```
data['a'] = data['a'].interpolate()
```

```
from fancyimpute import KNN
fill_knn = KNN(k=3).fit_transform(data)
data = pd.DataFrame(fill_knn)
print(data.head())
#out 
       0    1    2       3         4    5
0  111.0  0.0  2.0   360.0  4.000000  1.0
1  112.0  1.0  9.0  1080.0  3.000000  1.0
2  113.0  1.0  9.0  1080.0  2.000000  1.0
3  114.0  0.0  1.0   360.0 *3.862873 *1.0
4  115.0  0.0  1.0   270.0  5.000000  1.0

```

###  处理分类型特征：编码与哑变量

```
from sklearn.preprocessing import LabelEncoder

y = data.iloc[:,-1]                         #要输入的是标签，不是特征矩阵，所以允许一维

le = LabelEncoder()                         #实例化
le = le.fit(y)                              #导入数据
label = le.transform(y)                     #transform接口调取结果

le.classes_                                 #属性.classes_查看标签中究竟有多少类别
label                                       #查看获取的结果label

le.fit_transform(y)                         #也可以直接fit_transform一步到位

le.inverse_transform(label)                 #使用inverse_transform可以逆转

data.iloc[:,-1] = label                     #让标签等于我们运行出来的结果

data.head()
```

- ### 二值化与分段

```
data_2 = data.copy()

from sklearn.preprocessing import Binarizer
X = data_2.iloc[:,0].values.reshape(-1,1)               #类为特征专用，所以不能使用一维数组
transformer = Binarizer(threshold=30).fit_transform(X)

transformer
```

```
from sklearn.preprocessing import KBinsDiscretizer

X = data.iloc[:,0].values.reshape(-1,1) 
est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
est.fit_transform(X)

#查看转换后分的箱：变成了一列中的三箱
set(est.fit_transform(X).ravel())

est = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='uniform')
#查看转换后分的箱：变成了哑变量
est.fit_transform(X).toarray()
```

