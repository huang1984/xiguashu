管道机制在机器学习算法中得以应用的根源在于，参数集在新数据集（比如测试集）上的**重复使用**。

管道机制实现了对全部步骤的流式化封装和管理（**streaming workflows with pipelines**）。

```
mport pandas as pd
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/'
                 'breast-cancer-wisconsin/wdbc.data', header=None)
                                 # Breast Cancer Wisconsin dataset

X, y = df.values[:, 2:], df.values[:, 1]
                                # y为字符型标签
                                # 使用LabelEncoder类将其转换为0开始的数值型
encoder = LabelEncoder()
y = encoder.fit_transform(y)
                    >>> encoder.transform(['M', 'B'])
                    array([1, 0])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)

```

可放在Pipeline中的步骤可能有：

- 特征标准化是需要的，可作为第一个环节
- 既然是分类器，classifier也是少不了的，自然是最后一个环节
- 中间可加上比如数据降维（PCA）

```
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression

from sklearn.pipeline import Pipeline

pipe_lr = Pipeline([('sc', StandardScaler()),
                    ('pca', PCA(n_components=2)),
                    ('clf', LogisticRegression(random_state=1))
                    ])
pipe_lr.fit(X_train, y_train)
print('Test accuracy: %.3f' % pipe_lr.score(X_test, y_test))

                # Test accuracy: 0.947

```

 偏差描述的是算法的预测的平均值和真实值的关系（可以想象成算法的拟合能力如何），而方差描述的是同一个算法在不同数据集上的预测值和所有数据集上的平均预测值之间的关系（可以想象成算法的稳定性如何）。 

# 正则化（Regularization）

机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作 **ℓ1**

**ℓ1-norm** 和 **ℓ2-norm**，中文称作 ***L1正则化*** 和 ***L2正则化***，或者 ***L1范数*** 和 ***L2范数***。

L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。

![lasso regression](https://img-blog.csdn.net/20160904184228158)



![ridge regression](https://img-blog.csdn.net/20160904184314333)



- L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
- L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合

```
def L1Norm(l, theta):
    return  np.dot(np.abs(theta), np.ones(theta.size)) * l
 
def L1NormPartial(l, theta):
    return np.sign(theta) * l
def L2Norm(l, theta):
    return  np.dot(theta, theta) * l 
 
def L2NormPartial(l, theta):
    return theta * l
 def __Jfunction(self):        
        sum = 0
        
        for i in range(0, self.m):
            err = self.__error_dist(self.x[i], self.y[i])
            sum += np.dot(err, err)
        
        sum += Regularization.L2Norm(0.8, self.theta)
        
        return 1/(2 * self.m) * sum
def __partialderiv_J_func(self):
        sum = 0
 
        for i in range(0, self.m):
            err = self.__error_dist(self.x[i], self.y[i])
            sum += np.dot(self.x[i], err)
 
        sum += Regularization.L2NormPartial(0.8, self.theta)
   
        return 1/self.m * sum

```

